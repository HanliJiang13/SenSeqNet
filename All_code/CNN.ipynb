{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOZUNBaqK8FpQJKaPzjL7wF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"CvIxvTekH1jC"},"outputs":[],"source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader, TensorDataset\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import roc_curve, precision_recall_curve, accuracy_score, confusion_matrix, classification_report, roc_auc_score\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from tqdm import tqdm\n","\n","random_seed = 42\n","torch.manual_seed(random_seed)\n","torch.cuda.manual_seed(random_seed)\n","torch.cuda.manual_seed_all(random_seed)\n","np.random.seed(random_seed)\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = False\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f'Using device: {device}')\n","\n","# Load features and labels\n","X = np.load('/content/drive/MyDrive/esm_features_35M.npy')\n","y = np.load('/content/drive/MyDrive/labels_35M.npy')\n","\n","# Split the data into train and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=random_seed)\n","\n","# Reshape the features for CNN input\n","X_train = X_train.reshape(-1, 1, X_train.shape[1], 1)\n","X_test = X_test.reshape(-1, 1, X_test.shape[1], 1)\n","\n","# Convert to PyTorch tensors\n","X_train = torch.tensor(X_train, dtype=torch.float32)\n","X_test = torch.tensor(X_test, dtype=torch.float32)\n","y_train = torch.tensor(y_train, dtype=torch.long)\n","y_test = torch.tensor(y_test, dtype=torch.long)\n","\n","# Create DataLoader\n","train_dataset = TensorDataset(X_train, y_train)\n","test_dataset = TensorDataset(X_test, y_test)\n","train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n","\n","# Define the deeper CNN architecture with batch normalization and higher dropout\n","class DeeperCNNClassifier(nn.Module):\n","    def __init__(self, input_dim, num_classes):\n","        super(DeeperCNNClassifier, self).__init__()\n","        self.conv1 = nn.Conv2d(1, 79, kernel_size=(3, 1))\n","        self.bn1 = nn.BatchNorm2d(79)\n","        self.conv2 = nn.Conv2d(79, 111, kernel_size=(7, 1))\n","        self.bn2 = nn.BatchNorm2d(111)\n","        self.conv3 = nn.Conv2d(111, 180, kernel_size=(6, 1))\n","        self.bn3 = nn.BatchNorm2d(180)\n","        self.pool = nn.MaxPool2d(kernel_size=(2, 1))\n","\n","        # Calculate the size of the flattened features\n","        self.flatten_dim = self._get_flatten_dim(input_dim)\n","\n","        self.fc1 = nn.Linear(self.flatten_dim, 512)\n","        self.fc2 = nn.Linear(512, 256)\n","        self.fc3 = nn.Linear(256, num_classes)\n","        self.dropout = nn.Dropout(0.2978)\n","\n","    def _get_flatten_dim(self, input_dim):\n","        # Dummy forward pass to calculate the size of the flattened layer\n","        x = torch.zeros(1, 1, input_dim, 1)\n","        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n","        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n","        x = F.relu(self.bn3(self.conv3(x)))\n","        return x.numel()\n","\n","    def forward(self, x):\n","        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n","        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n","        x = F.relu(self.bn3(self.conv3(x)))\n","        x = x.view(x.size(0), -1)\n","        x = F.relu(self.fc1(x))\n","        x = self.dropout(x)\n","        x = F.relu(self.fc2(x))\n","        x = self.dropout(x)\n","        x = self.fc3(x)\n","        return F.log_softmax(x, dim=1)\n","\n","# Initialize the model, loss function, and optimizer\n","input_dim = X_train.shape[2]\n","num_classes = 2\n","model = DeeperCNNClassifier(input_dim, num_classes).to(device)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.AdamW(model.parameters(), lr=4.575e-05, weight_decay=1e-3)  # Added L2 regularization\n","scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n","\n","# Training loop with early stopping\n","n_epochs = 50\n","patience = 10\n","best_val_loss = float('inf')\n","epochs_no_improve = 0\n","\n","train_losses = []\n","val_losses = []\n","train_accuracies = []\n","val_accuracies = []\n","\n","# Training loop with early stopping\n","n_epochs = 50\n","patience = 5  # Number of epochs to wait for improvement before stopping\n","best_val_loss = float('inf')\n","early_stop_counter = 0\n","\n","train_losses = []\n","val_losses = []\n","train_accuracies = []\n","val_accuracies = []\n","\n","for epoch in range(n_epochs):\n","    model.train()\n","    train_loss = 0.0\n","    correct = 0\n","    total = 0\n","    for data, target in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}/{n_epochs}\", leave=False):\n","        data, target = data.to(device), target.to(device)\n","        optimizer.zero_grad()\n","        output = model(data)\n","        loss = criterion(output, target)\n","        loss.backward()\n","        optimizer.step()\n","        train_loss += loss.item() * data.size(0)\n","        _, predicted = torch.max(output, 1)\n","        correct += (predicted == target).sum().item()\n","        total += target.size(0)\n","\n","    train_loss /= total\n","    train_losses.append(train_loss)\n","    train_accuracy = correct / total\n","    train_accuracies.append(train_accuracy)\n","\n","    model.eval()\n","    val_loss = 0.0\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for data, target in tqdm(test_loader, desc=f\"Validating Epoch {epoch+1}/{n_epochs}\", leave=False):\n","            data, target = data.to(device), target.to(device)\n","            output = model(data)\n","            loss = criterion(output, target)\n","            val_loss += loss.item() * data.size(0)\n","            _, predicted = torch.max(output, 1)\n","            correct += (predicted == target).sum().item()\n","            total += target.size(0)\n","\n","    val_loss /= total\n","    val_losses.append(val_loss)\n","    val_accuracy = correct / total\n","    val_accuracies.append(val_accuracy)\n","\n","    print(f'Epoch {epoch+1}/{n_epochs}')\n","    print(f'Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n","\n","    # Early stopping\n","    if val_loss < best_val_loss:\n","        best_val_loss = val_loss\n","        early_stop_counter = 0\n","        model_save_path = '/content/drive/MyDrive/FinalTest/best_cnn_model.pth'\n","        torch.save(model.state_dict(), model_save_path)\n","        print(\"  Best model saved to Google Drive!\")\n","    else:\n","        early_stop_counter += 1\n","        if early_stop_counter >= patience:\n","            print(\"Early stopping triggered\")\n","            break\n","\n","\n","# Load the best model\n","print(\"Loading the best model from Google Drive...\")\n","model.load_state_dict(torch.load('/content/drive/MyDrive/FinalTest/best_cnn_model.pth'))\n","\n","# Evaluate on the test set\n","model.eval()\n","test_loss = 0.0\n","correct = 0\n","y_pred_prob = []\n","y_true = []\n","\n","print(\"Evaluating on the test set...\")\n","with torch.no_grad():\n","    for data, target in tqdm(test_loader, desc=\"Testing\", leave=False):\n","        data, target = data.to(device), target.to(device)\n","        output = model(data)\n","        loss = criterion(output, target)\n","        test_loss += loss.item() * data.size(0)\n","        _, predicted = torch.max(output, 1)\n","        correct += (predicted == target).sum().item()\n","        y_pred_prob.extend(output[:, 1].cpu().numpy())\n","        y_true.extend(target.cpu().numpy())\n","\n","test_loss /= len(test_loader.dataset)\n","test_accuracy = correct / len(test_loader.dataset)\n","print(f'Test Accuracy: {test_accuracy:.4f}')\n","\n","# Calculate additional metrics\n","roc_auc = roc_auc_score(y_true, y_pred_prob)\n","fpr, tpr, _ = roc_curve(y_true, y_pred_prob)\n","precision, recall, _ = precision_recall_curve(y_true, y_pred_prob)\n","\n","# Plot Confusion Matrix\n","y_pred = np.argmax(model(torch.tensor(X_test, dtype=torch.float32).to(device)).cpu().detach().numpy(), axis=1)\n","conf_matrix = confusion_matrix(y_test, y_pred)\n","plt.figure(figsize=(8, 6))\n","sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])\n","plt.xlabel('Predicted')\n","plt.ylabel('True')\n","plt.title('Confusion Matrix')\n","plt.show()\n","\n","# Classification Report\n","print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n","\n","# ROC Curve and AUC\n","plt.figure(figsize=(12, 6))\n","plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n","plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n","plt.xlim([0.0, 1.0])\n","plt.ylim([0.0, 1.05])\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('True Positive Rate')\n","plt.title('Receiver Operating Characteristic')\n","plt.legend(loc='lower right')\n","plt.show()\n","\n","# Precision-Recall Curve\n","plt.figure(figsize=(12, 6))\n","plt.plot(recall, precision, color='blue', lw=2)\n","plt.xlabel('Recall')\n","plt.ylabel('Precision')\n","plt.title('Precision-Recall Curve')\n","plt.show()\n"]},{"cell_type":"code","source":["import optuna\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader, TensorDataset\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import roc_auc_score\n","from tqdm import tqdm\n","\n","# Set random seeds for reproducibility\n","random_seed = 42\n","torch.manual_seed(random_seed)\n","torch.cuda.manual_seed(random_seed)\n","torch.cuda.manual_seed_all(random_seed)\n","np.random.seed(random_seed)\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = False\n","\n","# Check if GPU is available and set device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f'Using device: {device}')\n","\n","# Load features and labels\n","X = np.load('/content/drive/MyDrive/esm_features_35M.npy')\n","y = np.load('/content/drive/MyDrive/labels_35M.npy')\n","\n","# Split the data into train and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=random_seed)\n","\n","# Reshape the features for CNN input (e.g., (num_samples, num_channels, height, width))\n","X_train = X_train.reshape(-1, 1, X_train.shape[1], 1)\n","X_test = X_test.reshape(-1, 1, X_test.shape[1], 1)\n","\n","# Convert to PyTorch tensors\n","X_train = torch.tensor(X_train, dtype=torch.float32)\n","X_test = torch.tensor(X_test, dtype=torch.float32)\n","y_train = torch.tensor(y_train, dtype=torch.long)\n","y_test = torch.tensor(y_test, dtype=torch.long)\n","\n","# Create DataLoader\n","train_dataset = TensorDataset(X_train, y_train)\n","test_dataset = TensorDataset(X_test, y_test)\n","\n","# Define the objective function for Optuna\n","def objective(trial):\n","    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128])\n","    learning_rate = trial.suggest_float('lr', 1e-5, 1e-3, log=True)\n","    dropout_rate = trial.suggest_float('dropout_rate', 0.2, 0.7)\n","    num_filters1 = trial.suggest_int('num_filters1', 32, 128)\n","    num_filters2 = trial.suggest_int('num_filters2', 64, 256)\n","    num_filters3 = trial.suggest_int('num_filters3', 128, 512)\n","    kernel_size1 = trial.suggest_int('kernel_size1', 3, 7)\n","    kernel_size2 = trial.suggest_int('kernel_size2', 3, 7)\n","    kernel_size3 = trial.suggest_int('kernel_size3', 3, 7)\n","\n","    # Create DataLoader\n","    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","\n","    class DeeperCNNClassifier(nn.Module):\n","        def __init__(self, input_dim, num_classes, dropout_rate):\n","            super(DeeperCNNClassifier, self).__init__()\n","            self.conv1 = nn.Conv2d(1, num_filters1, kernel_size=(kernel_size1, 1))\n","            self.bn1 = nn.BatchNorm2d(num_filters1)\n","            self.conv2 = nn.Conv2d(num_filters1, num_filters2, kernel_size=(kernel_size2, 1))\n","            self.bn2 = nn.BatchNorm2d(num_filters2)\n","            self.conv3 = nn.Conv2d(num_filters2, num_filters3, kernel_size=(kernel_size3, 1))\n","            self.bn3 = nn.BatchNorm2d(num_filters3)\n","            self.pool = nn.MaxPool2d(kernel_size=(2, 1))\n","\n","            # Calculate the size of the flattened features\n","            self.flatten_dim = self._get_flatten_dim(input_dim)\n","\n","            self.fc1 = nn.Linear(self.flatten_dim, 512)\n","            self.fc2 = nn.Linear(512, 256)\n","            self.fc3 = nn.Linear(256, num_classes)\n","            self.dropout = nn.Dropout(dropout_rate)\n","\n","        def _get_flatten_dim(self, input_dim):\n","            # Dummy forward pass to calculate the size of the flattened layer\n","            x = torch.zeros(1, 1, input_dim, 1)\n","            x = self.pool(F.relu(self.bn1(self.conv1(x))))\n","            x = self.pool(F.relu(self.bn2(self.conv2(x))))\n","            x = F.relu(self.bn3(self.conv3(x)))\n","            return x.numel()\n","\n","        def forward(self, x):\n","            x = self.pool(F.relu(self.bn1(self.conv1(x))))\n","            x = self.pool(F.relu(self.bn2(self.conv2(x))))\n","            x = F.relu(self.bn3(self.conv3(x)))\n","            x = x.view(x.size(0), -1)\n","            x = F.relu(self.fc1(x))\n","            x = self.dropout(x)\n","            x = F.relu(self.fc2(x))\n","            x = self.dropout(x)\n","            x = self.fc3(x)\n","            return F.log_softmax(x, dim=1)\n","\n","    # Initialize the model, loss function, and optimizer\n","    input_dim = X_train.shape[2]\n","    num_classes = 2\n","    model = DeeperCNNClassifier(input_dim, num_classes, dropout_rate).to(device)\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n","\n","    # Training loop with early stopping\n","    n_epochs = 20  # Keep it small for faster optimization\n","    best_val_loss = float('inf')\n","    early_stop_counter = 0\n","    patience = 5\n","\n","    for epoch in range(n_epochs):\n","        model.train()\n","        train_loss = 0.0\n","        correct = 0\n","        total = 0\n","        for data, target in train_loader:\n","            data, target = data.to(device), target.to(device)\n","            optimizer.zero_grad()\n","            output = model(data)\n","            loss = criterion(output, target)\n","            loss.backward()\n","            optimizer.step()\n","            train_loss += loss.item() * data.size(0)\n","            _, predicted = torch.max(output, 1)\n","            correct += (predicted == target).sum().item()\n","            total += target.size(0)\n","\n","        train_loss /= total\n","\n","        model.eval()\n","        val_loss = 0.0\n","        correct = 0\n","        total = 0\n","        with torch.no_grad():\n","            for data, target in test_loader:\n","                data, target = data.to(device), target.to(device)\n","                output = model(data)\n","                loss = criterion(output, target)\n","                val_loss += loss.item() * data.size(0)\n","                _, predicted = torch.max(output, 1)\n","                correct += (predicted == target).sum().item()\n","                total += target.size(0)\n","\n","        val_loss /= total\n","\n","        if val_loss < best_val_loss:\n","            best_val_loss = val_loss\n","            early_stop_counter = 0\n","        else:\n","            early_stop_counter += 1\n","            if early_stop_counter >= patience:\n","                break\n","\n","    # Calculate ROC AUC as the optimization target\n","    model.eval()\n","    y_pred_prob = []\n","    y_true = []\n","\n","    with torch.no_grad():\n","        for data, target in test_loader:\n","            data, target = data.to(device), target.to(device)\n","            output = model(data)\n","            y_pred_prob.extend(output[:, 1].cpu().numpy())\n","            y_true.extend(target.cpu().numpy())\n","\n","    roc_auc = roc_auc_score(y_true, y_pred_prob)\n","    return roc_auc\n","\n","# Step 3: Optimize Hyperparameters with Optuna\n","study = optuna.create_study(direction='maximize')\n","study.optimize(objective, n_trials=50)\n","\n","# Step 4: Print the Best Hyperparameters\n","print('Best hyperparameters:', study.best_params)\n","print('Best AUC score:', study.best_value)\n"],"metadata":{"id":"CTwCnJ-jIlnt"},"execution_count":null,"outputs":[]}]}