{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPMjU/UCBlsa85zrDjUsr6X"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"SMUoZNEuKRdX"},"outputs":[],"source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, TensorDataset\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve, precision_recall_curve, confusion_matrix, classification_report\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from tqdm import tqdm\n","import torch.nn.functional as F\n","\n","# Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Set random seeds for reproducibility\n","random_seed = 42\n","torch.manual_seed(random_seed)\n","torch.cuda.manual_seed(random_seed)\n","torch.cuda.manual_seed_all(random_seed)\n","np.random.seed(random_seed)\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = False\n","\n","# Check if GPU is available and set device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f'Using device: {device}')\n","\n","# Load features and labels from Google Drive\n","X = np.load('/content/drive/MyDrive/esm_features_35M.npy')\n","y = np.load('/content/drive/MyDrive/labels_35M.npy')\n","\n","# Split the data into train and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=random_seed)\n","\n","# Reshape the features for LSTM input (e.g., (num_samples, seq_len, feature_dim))\n","seq_len = 1  # This should match your sequence length if it's different\n","# X_train = X_train.reshape(-1, seq_len, X_train.shape[1])\n","# X_test = X_test.reshape(-1, seq_len, X_test.shape[1])\n","\n","# Convert to PyTorch tensors\n","X_train = torch.tensor(X_train, dtype=torch.float32)\n","X_test = torch.tensor(X_test, dtype=torch.float32)\n","y_train = torch.tensor(y_train, dtype=torch.long)\n","y_test = torch.tensor(y_test, dtype=torch.long)\n","\n","# Create DataLoader\n","train_dataset = TensorDataset(X_train, y_train)\n","test_dataset = TensorDataset(X_test, y_test)\n","batch_size = 32\n","\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","\n","class PARA_CNNLSTM_Classifier(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, num_layers, num_classes, dropout_rate):\n","        super(PARA_CNNLSTM_Classifier, self).__init__()\n","        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout_rate, bidirectional=True)\n","        self.bn = nn.BatchNorm1d(hidden_dim * 2)\n","        self.conv1 = nn.Conv2d(in_channels=1, out_channels=76, kernel_size=(6, 1), padding=(1, 0))\n","        self.bn1 = nn.BatchNorm2d(76)\n","        self.pool = nn.MaxPool2d(kernel_size=(2, 1), stride=(2, 1))\n","        self.conv2 = nn.Conv2d(in_channels=76, out_channels=111, kernel_size=(4, 1), padding=(1, 0))\n","        self.bn2 = nn.BatchNorm2d(111)\n","        self.conv3 = nn.Conv2d(in_channels=111, out_channels=487, kernel_size=(5, 1), padding=(1, 0))\n","        self.bn3 = nn.BatchNorm2d(487)\n","\n","        self.flatten_dim = self._get_flatten_dim(input_dim)\n","        self.fc = nn.Linear(self.flatten_dim, num_classes)\n","        self.dropout = nn.Dropout(dropout_rate)\n","\n","    def _get_flatten_dim(self, input_dim):\n","        h0 = torch.zeros(num_layers * 2, batch_size, hidden_dim)\n","        c0 = torch.zeros(num_layers * 2, batch_size, hidden_dim)\n","        in_lstm = torch.ones(batch_size, 1, input_dim)\n","        in_cnn = torch.zeros(batch_size, 1, input_dim, 1)\n","        out_cnn = self.conv1(in_cnn)\n","        out_cnn = self.bn1(out_cnn)\n","        out_cnn = F.relu(out_cnn)\n","        out_cnn = self.pool(out_cnn)\n","        out_cnn = self.conv2(out_cnn)\n","        out_cnn = self.bn2(out_cnn)\n","        out_cnn = F.relu(out_cnn)\n","        out_cnn = self.pool(out_cnn)\n","        out_cnn = self.conv3(out_cnn)\n","        out_cnn = self.bn3(out_cnn)\n","        out_cnn = F.relu(out_cnn)\n","        out_cnn = self.pool(out_cnn)\n","        out_cnn = out_cnn.reshape(batch_size,-1)\n","        out_lstm, _ = self.lstm(in_lstm, (h0, c0))\n","        out_lstm = out_lstm[:, -1, :]\n","        out_lstm = self.bn(out_lstm)\n","        out_combine = torch.cat([out_cnn, out_lstm], dim=1)\n","        return out_combine.size(1)\n","\n","    def forward(self, x):\n","        # CNN\n","        in_cnn = x.reshape(-1, 1, X_train.shape[1], 1)\n","        in_lstm = x.reshape(-1, seq_len, X_train.shape[1])\n","        out_cnn = self.conv1(in_cnn)\n","        out_cnn = self.bn1(out_cnn)\n","        out_cnn = F.relu(out_cnn)\n","        out_cnn = self.pool(out_cnn)\n","        out_cnn = self.conv2(out_cnn)\n","        out_cnn = self.bn2(out_cnn)\n","        out_cnn = F.relu(out_cnn)\n","        out_cnn = self.pool(out_cnn)\n","        out_cnn = self.conv3(out_cnn)\n","        out_cnn = self.bn3(out_cnn)\n","        out_cnn = F.relu(out_cnn)\n","        out_cnn = self.pool(out_cnn)\n","        out_cnn = out_cnn.reshape(out_cnn.size(0),-1)\n","        # LSTM\n","        h0 = torch.zeros(self.lstm.num_layers * 2, in_lstm.size(0), self.lstm.hidden_size).to(device)\n","        c0 = torch.zeros(self.lstm.num_layers * 2, in_lstm.size(0), self.lstm.hidden_size).to(device)\n","        out_lstm, _ = self.lstm(in_lstm, (h0, c0))\n","        out_lstm = out_lstm[:, -1, :]\n","        out_lstm = self.bn(out_lstm)\n","        out_lstm = self.dropout(out_lstm)\n","        out_combine = torch.cat([out_cnn, out_lstm], dim=1)\n","        out = self.fc(out_combine)\n","        return F.log_softmax(out, dim=1)\n","\n","# Initialize the improved LSTM model\n","input_dim = X_train.reshape(-1, seq_len, X_train.shape[1]).shape[2]\n","hidden_dim = 255\n","num_layers = 4\n","dropout_rate = 0.5443207069354133\n","learning_rate_cnn = 4.579041182623139e-05\n","learning_rate_fc = 1.0172301244333337e-05\n","learning_rate = 0.0005533164271008041\n","num_classes = 2\n","\n","model = PARA_CNNLSTM_Classifier(input_dim, hidden_dim, num_layers, num_classes, dropout_rate).to(device)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam([\n","    {'params': model.conv1.parameters(), 'lr': learning_rate_cnn},\n","    {'params': model.bn1.parameters(), 'lr': learning_rate_cnn},\n","    {'params': model.conv2.parameters(), 'lr': learning_rate_cnn},\n","    {'params': model.bn2.parameters(), 'lr': learning_rate_cnn},\n","    {'params': model.conv3.parameters(), 'lr': learning_rate_cnn},\n","    {'params': model.bn3.parameters(), 'lr': learning_rate_cnn},\n","    {'params': model.lstm.parameters(), 'lr': learning_rate},\n","    {'params': model.fc.parameters(), 'lr': learning_rate_fc},  # Assuming you want the same LR as CNN for the final FC layer\n","    {'params': model.bn.parameters(), 'lr': learning_rate_fc},  # Assuming the same for the batch norm layer after LSTM\n","], lr=learning_rate_cnn)\n","\n","\n","# Training loop with early stopping\n","n_epochs = 30\n","patience = 5  # Number of epochs to wait for improvement before stopping\n","best_val_loss = float('inf')\n","early_stop_counter = 0\n","\n","train_losses = []\n","val_losses = []\n","train_accuracies = []\n","val_accuracies = []\n","\n","for epoch in range(n_epochs):\n","    model.train()\n","    train_loss = 0.0\n","    correct = 0\n","    total = 0\n","    for data, target in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}/{n_epochs}\", leave=False):\n","        data, target = data.to(device), target.to(device)\n","        optimizer.zero_grad()\n","        output = model(data)\n","        loss = criterion(output, target)\n","        loss.backward()\n","        optimizer.step()\n","        train_loss += loss.item() * data.size(0)\n","        _, predicted = torch.max(output, 1)\n","        correct += (predicted == target).sum().item()\n","        total += target.size(0)\n","\n","    train_loss /= total\n","    train_losses.append(train_loss)\n","    train_accuracy = correct / total\n","    train_accuracies.append(train_accuracy)\n","\n","    model.eval()\n","    val_loss = 0.0\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for data, target in tqdm(test_loader, desc=f\"Validating Epoch {epoch+1}/{n_epochs}\", leave=False):\n","            data, target = data.to(device), target.to(device)\n","            output = model(data)\n","            loss = criterion(output, target)\n","            val_loss += loss.item() * data.size(0)\n","            _, predicted = torch.max(output, 1)\n","            correct += (predicted == target).sum().item()\n","            total += target.size(0)\n","\n","    val_loss /= total\n","    val_losses.append(val_loss)\n","    val_accuracy = correct / total\n","    val_accuracies.append(val_accuracy)\n","\n","    print(f'Epoch {epoch+1}/{n_epochs}')\n","    print(f'Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n","\n","    # Early stopping\n","    if val_loss < best_val_loss:\n","        best_val_loss = val_loss\n","        early_stop_counter = 0\n","        model_save_path = '/content/drive/MyDrive/best_PARA_CNNLSTM_model_35M.pth'\n","        torch.save(model.state_dict(), model_save_path)\n","        print(\"  Best model saved to Google Drive!\")\n","    else:\n","        early_stop_counter += 1\n","        if early_stop_counter >= patience:\n","            print(\"Early stopping triggered\")\n","            break\n","\n","# Load the best model\n","print(\"Loading the best model from Google Drive...\")\n","model.load_state_dict(torch.load('/content/drive/MyDrive/best_PARA_CNNLSTM_model_35M.pth'))\n","\n","# Evaluate on the test set\n","model.eval()\n","test_loss = 0.0\n","correct = 0\n","y_pred_prob = []\n","y_true = []\n","\n","print(\"Evaluating on the test set...\")\n","with torch.no_grad():\n","    for data, target in tqdm(test_loader, desc=\"Testing\", leave=False):\n","        data, target = data.to(device), target.to(device)\n","        output = model(data)\n","        loss = criterion(output, target)\n","        test_loss += loss.item() * data.size(0)\n","        _, predicted = torch.max(output, 1)\n","        correct += (predicted == target).sum().item()\n","        y_pred_prob.extend(output[:, 1].cpu().numpy())\n","        y_true.extend(target.cpu().numpy())\n","\n","test_loss /= len(test_loader.dataset)\n","test_accuracy = correct / len(test_loader.dataset)\n","print(f'Test Accuracy: {test_accuracy:.4f}')\n","\n","# Calculate additional metrics\n","roc_auc = roc_auc_score(y_true, y_pred_prob)\n","fpr, tpr, _ = roc_curve(y_true, y_pred_prob)\n","precision, recall, _ = precision_recall_curve(y_true, y_pred_prob)\n","\n","# Plot Confusion Matrix\n","y_pred = np.argmax(model(torch.tensor(X_test, dtype=torch.float32).to(device)).cpu().detach().numpy(), axis=1)\n","conf_matrix = confusion_matrix(y_test, y_pred)\n","plt.figure(figsize=(8, 6))\n","sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])\n","plt.xlabel('Predicted')\n","plt.ylabel('True')\n","plt.title('Confusion Matrix')\n","plt.show()\n","\n","# Classification Report\n","print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n","\n","# ROC Curve and AUC\n","plt.figure(figsize=(12, 6))\n","plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n","plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n","plt.xlim([0.0, 1.0])\n","plt.ylim([0.0, 1.05])\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('True Positive Rate')\n","plt.title('Receiver Operating Characteristic')\n","plt.legend(loc='lower right')\n","plt.show()\n","\n","# Precision-Recall Curve\n","plt.figure(figsize=(12, 6))\n","plt.plot(recall, precision, color='blue', lw=2)\n","plt.xlabel('Recall')\n","plt.ylabel('Precision')\n","plt.title('Precision-Recall Curve')\n","plt.show()\n"]},{"cell_type":"code","source":["import optuna\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, TensorDataset\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve, precision_recall_curve, confusion_matrix, classification_report\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from tqdm import tqdm\n","import torch.nn.functional as F\n","\n","# Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Set random seeds for reproducibility\n","random_seed = 42\n","torch.manual_seed(random_seed)\n","torch.cuda.manual_seed(random_seed)\n","torch.cuda.manual_seed_all(random_seed)\n","np.random.seed(random_seed)\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = False\n","\n","# Check if GPU is available and set device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f'Using device: {device}')\n","\n","# Load features and labels from Google Drive\n","X = np.load('/content/drive/MyDrive/esm_features_35M.npy')\n","y = np.load('/content/drive/MyDrive/labels_35M.npy')\n","\n","# Split the data into train and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=random_seed)\n","\n","# Reshape the features for LSTM input (e.g., (num_samples, seq_len, feature_dim))\n","seq_len = 1  # This should match your sequence length if it's different\n","# X_train = X_train.reshape(-1, 1, X_train.shape[1], 1)\n","# X_test = X_test.reshape(-1, 1, X_test.shape[1], 1)\n","# X_train = X_train.reshape(-1, seq_len, X_train.shape[1])\n","# X_test = X_test.reshape(-1, seq_len, X_test.shape[1])\n","# Convert to PyTorch tensors\n","X_train = torch.tensor(X_train, dtype=torch.float32)\n","X_test = torch.tensor(X_test, dtype=torch.float32)\n","y_train = torch.tensor(y_train, dtype=torch.long)\n","y_test = torch.tensor(y_test, dtype=torch.long)\n","\n","# Create DataLoader\n","train_dataset = TensorDataset(X_train, y_train)\n","test_dataset = TensorDataset(X_test, y_test)\n","def objective(trial):\n","    # Suggest hyperparameters\n","    hidden_dim = 255\n","    num_layers = 4\n","    dropout_rate = trial.suggest_float('dropout_rate', 0.3, 0.6)\n","    learning_rate = 0.0005533164271008041\n","    learning_rate_cnn = 4.579041182623139e-05\n","    learning_rate_fc = trial.suggest_float('lr', 1e-5, 1e-3, log=True)\n","    num_filters1 = 76\n","    num_filters2 = 111\n","    num_filters3 = 487\n","    kernel_size1 = 6\n","    kernel_size2 = 4\n","    kernel_size3 = 5\n","    batch_size = trial.suggest_categorical('batch_size', [32, 64])\n","\n","    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","\n","\n","    class PARA_CNNLSTM_Classifier(nn.Module):\n","        def __init__(self, input_dim, hidden_dim, num_layers, num_classes, dropout_rate):\n","            super(PARA_CNNLSTM_Classifier, self).__init__()\n","            self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout_rate, bidirectional=True)\n","            self.bn = nn.BatchNorm1d(hidden_dim * 2)\n","            self.conv1 = nn.Conv2d(in_channels=1, out_channels=num_filters1, kernel_size=(kernel_size1, 1), padding=(1, 0))\n","            self.bn1 = nn.BatchNorm2d(num_filters1)\n","            self.pool = nn.MaxPool2d(kernel_size=(2, 1), stride=(2, 1))\n","            self.conv2 = nn.Conv2d(in_channels=num_filters1, out_channels=num_filters2, kernel_size=(kernel_size2, 1), padding=(1, 0))\n","            self.bn2 = nn.BatchNorm2d(num_filters2)\n","            self.conv3 = nn.Conv2d(in_channels=num_filters2, out_channels=num_filters3, kernel_size=(kernel_size3, 1), padding=(1, 0))\n","            self.bn3 = nn.BatchNorm2d(num_filters3)\n","\n","            # Compute the output dimension size for the final fully connected layer\n","            # Assuming each dimension is halved by pooling once\n","            self.flatten_dim = self._get_flatten_dim(input_dim)\n","            self.fc = nn.Linear(self.flatten_dim, num_classes)\n","            self.dropout = nn.Dropout(dropout_rate)\n","\n","        def _get_flatten_dim(self, input_dim):\n","            h0 = torch.zeros(num_layers * 2, batch_size, hidden_dim)\n","            c0 = torch.zeros(num_layers * 2, batch_size, hidden_dim)\n","            in_lstm = torch.ones(batch_size, 1, input_dim)\n","            in_cnn = torch.zeros(batch_size, 1, input_dim, 1)\n","            out_cnn = self.conv1(in_cnn)\n","            out_cnn = self.bn1(out_cnn)\n","            out_cnn = F.relu(out_cnn)\n","            out_cnn = self.pool(out_cnn)\n","            out_cnn = self.conv2(out_cnn)\n","            out_cnn = self.bn2(out_cnn)\n","            out_cnn = F.relu(out_cnn)\n","            out_cnn = self.pool(out_cnn)\n","            out_cnn = self.conv3(out_cnn)\n","            out_cnn = self.bn3(out_cnn)\n","            out_cnn = F.relu(out_cnn)\n","            out_cnn = self.pool(out_cnn)\n","            out_cnn = out_cnn.reshape(batch_size,-1)\n","            out_lstm, _ = self.lstm(in_lstm, (h0, c0))\n","            out_lstm = out_lstm[:, -1, :]\n","            out_lstm = self.bn(out_lstm)\n","            out_combine = torch.cat([out_cnn, out_lstm], dim=1)\n","            return out_combine.size(1)\n","\n","        def forward(self, x):\n","            # CNN\n","            in_cnn = x.reshape(-1, 1, X_train.shape[1], 1)\n","            in_lstm = x.reshape(-1, seq_len, X_train.shape[1])\n","            out_cnn = self.conv1(in_cnn)\n","            out_cnn = self.bn1(out_cnn)\n","            out_cnn = F.relu(out_cnn)\n","            out_cnn = self.pool(out_cnn)\n","            out_cnn = self.conv2(out_cnn)\n","            out_cnn = self.bn2(out_cnn)\n","            out_cnn = F.relu(out_cnn)\n","            out_cnn = self.pool(out_cnn)\n","            out_cnn = self.conv3(out_cnn)\n","            out_cnn = self.bn3(out_cnn)\n","            out_cnn = F.relu(out_cnn)\n","            out_cnn = self.pool(out_cnn)\n","            out_cnn = out_cnn.reshape(out_cnn.size(0),-1)\n","            #LSTM\n","            h0 = torch.zeros(self.lstm.num_layers * 2, in_lstm.size(0), self.lstm.hidden_size).to(device)\n","            c0 = torch.zeros(self.lstm.num_layers * 2, in_lstm.size(0), self.lstm.hidden_size).to(device)\n","            # out = out.squeeze(-1)\n","            # out = out.permute(0, 2, 1)\n","            out_lstm, _ = self.lstm(in_lstm, (h0, c0))\n","            out_lstm = out_lstm[:, -1, :]\n","            out_lstm = self.bn(out_lstm)\n","            out_lstm = self.dropout(out_lstm)\n","            out_combine = torch.cat([out_cnn, out_lstm], dim=1)\n","            out = self.fc(out_combine)\n","            return F.log_softmax(out, dim=1)\n","\n","    # Initialize the improved LSTM model\n","    input_dim = X_train.reshape(-1, seq_len, X_train.shape[1]).shape[2]\n","    num_classes = 2\n","\n","    model = PARA_CNNLSTM_Classifier(input_dim, hidden_dim, num_layers, num_classes, dropout_rate).to(device)\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.Adam([\n","        {'params': model.conv1.parameters(), 'lr': learning_rate_cnn},\n","        {'params': model.bn1.parameters(), 'lr': learning_rate_cnn},\n","        {'params': model.conv2.parameters(), 'lr': learning_rate_cnn},\n","        {'params': model.bn2.parameters(), 'lr': learning_rate_cnn},\n","        {'params': model.conv3.parameters(), 'lr': learning_rate_cnn},\n","        {'params': model.bn3.parameters(), 'lr': learning_rate_cnn},\n","        {'params': model.lstm.parameters(), 'lr': learning_rate},\n","        {'params': model.fc.parameters(), 'lr': learning_rate_fc},  # Assuming you want the same LR as CNN for the final FC layer\n","        {'params': model.bn.parameters(), 'lr': learning_rate_fc},  # Assuming the same for the batch norm layer after LSTM\n","    ], lr=learning_rate_cnn)\n","\n","\n","    # Training loop with early stopping\n","    n_epochs = 30\n","    patience = 5  # Number of epochs to wait for improvement before stopping\n","    best_val_loss = float('inf')\n","    early_stop_counter = 0\n","\n","    train_losses = []\n","    val_losses = []\n","    train_accuracies = []\n","    val_accuracies = []\n","\n","    for epoch in range(n_epochs):\n","        model.train()\n","        train_loss = 0.0\n","        correct = 0\n","        total = 0\n","        for data, target in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}/{n_epochs}\", leave=False):\n","            data, target = data.to(device), target.to(device)\n","            optimizer.zero_grad()\n","            output = model(data)\n","            loss = criterion(output, target)\n","            loss.backward()\n","            optimizer.step()\n","            train_loss += loss.item() * data.size(0)\n","            _, predicted = torch.max(output, 1)\n","            correct += (predicted == target).sum().item()\n","            total += target.size(0)\n","\n","        train_loss /= total\n","        train_losses.append(train_loss)\n","        train_accuracy = correct / total\n","        train_accuracies.append(train_accuracy)\n","\n","        model.eval()\n","        val_loss = 0.0\n","        correct = 0\n","        total = 0\n","        with torch.no_grad():\n","            for data, target in tqdm(test_loader, desc=f\"Validating Epoch {epoch+1}/{n_epochs}\", leave=False):\n","                data, target = data.to(device), target.to(device)\n","                output = model(data)\n","                loss = criterion(output, target)\n","                val_loss += loss.item() * data.size(0)\n","                _, predicted = torch.max(output, 1)\n","                correct += (predicted == target).sum().item()\n","                total += target.size(0)\n","\n","        val_loss /= total\n","        val_losses.append(val_loss)\n","        val_accuracy = correct / total\n","        val_accuracies.append(val_accuracy)\n","\n","        print(f'Epoch {epoch+1}/{n_epochs}')\n","        print(f'Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n","\n","        # Early stopping\n","        if val_loss < best_val_loss:\n","            best_val_loss = val_loss\n","            early_stop_counter = 0\n","            model_save_path = '/content/drive/MyDrive/best_PARA_CNNLSTM_model_35M.pth'\n","            torch.save(model.state_dict(), model_save_path)\n","            print(\"  Best model saved to Google Drive!\")\n","        else:\n","            early_stop_counter += 1\n","            if early_stop_counter >= 5:\n","                print(\"Early stopping triggered\")\n","                break\n","    # Calculate ROC AUC as the optimization target\n","    model.eval()\n","    y_pred_prob = []\n","    y_true = []\n","\n","    with torch.no_grad():\n","        for data, target in test_loader:\n","            data, target = data.to(device), target.to(device)\n","            output = model(data)\n","            y_pred_prob.extend(output[:, 1].cpu().numpy())\n","            y_true.extend(target.cpu().numpy())\n","\n","    roc_auc = roc_auc_score(y_true, y_pred_prob)\n","    return roc_auc\n","study = optuna.create_study(direction='maximize')\n","study.optimize(objective, n_trials=50)\n","# Step 4: Print the Best Hyperparameters\n","print('Best hyperparameters:', study.best_params)\n","print('Best AUC score:', study.best_value)\n","# Load the best model\n","print(\"Loading the best model from Google Drive...\")\n","model.load_state_dict(torch.load('/content/drive/MyDrive/best_PARA_CNNLSTM_model_35M.pth'))\n","\n","# Evaluate on the test set\n","model.eval()\n","test_loss = 0.0\n","correct = 0\n","y_pred_prob = []\n","y_true = []\n","\n","print(\"Evaluating on the test set...\")\n","with torch.no_grad():\n","    for data, target in tqdm(test_loader, desc=\"Testing\", leave=False):\n","        data, target = data.to(device), target.to(device)\n","        output = model(data)\n","        loss = criterion(output, target)\n","        test_loss += loss.item() * data.size(0)\n","        _, predicted = torch.max(output, 1)\n","        correct += (predicted == target).sum().item()\n","        y_pred_prob.extend(output[:, 1].cpu().numpy())\n","        y_true.extend(target.cpu().numpy())\n","\n","test_loss /= len(test_loader.dataset)\n","test_accuracy = correct / len(test_loader.dataset)\n","print(f'Test Accuracy: {test_accuracy:.4f}')\n","\n","# Calculate additional metrics\n","roc_auc = roc_auc_score(y_true, y_pred_prob)\n","fpr, tpr, _ = roc_curve(y_true, y_pred_prob)\n","precision, recall, _ = precision_recall_curve(y_true, y_pred_prob)\n","\n","# Plot Confusion Matrix\n","y_pred = np.argmax(model(torch.tensor(X_test, dtype=torch.float32).to(device)).cpu().detach().numpy(), axis=1)\n","conf_matrix = confusion_matrix(y_test, y_pred)\n","plt.figure(figsize=(8, 6))\n","sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])\n","plt.xlabel('Predicted')\n","plt.ylabel('True')\n","plt.title('Confusion Matrix')\n","plt.show()\n","\n","# Classification Report\n","print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n","\n","# ROC Curve and AUC\n","plt.figure(figsize=(12, 6))\n","plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n","plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n","plt.xlim([0.0, 1.0])\n","plt.ylim([0.0, 1.05])\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('True Positive Rate')\n","plt.title('Receiver Operating Characteristic')\n","plt.legend(loc='lower right')\n","plt.show()\n","\n","# Precision-Recall Curve\n","plt.figure(figsize=(12, 6))\n","plt.plot(recall, precision, color='blue', lw=2)\n","plt.xlabel('Recall')\n","plt.ylabel('Precision')\n","plt.title('Precision-Recall Curve')\n","plt.show()\n"],"metadata":{"id":"lZWhkT65KkYp"},"execution_count":null,"outputs":[]}]}