{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d79f4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from Bio import SeqIO\n",
    "import torch\n",
    "import esm\n",
    "\n",
    "# Reproducibility\n",
    "random_seed = 42\n",
    "torch.manual_seed(random_seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Hardware + file system configuration\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "PROJECT_ROOT = Path('/content')\n",
    "print(f\"Using device: {device}\")\n",
    "POS_FASTA = PROJECT_ROOT / 'reps_30_rep_seq_pos.fasta'\n",
    "NEG_FASTA = PROJECT_ROOT / 'reps_30_rep_seq_neg.fasta'\n",
    "OUTPUT_DIR = PROJECT_ROOT / 'esm_outputs'\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MAX_SEQUENCES_PER_BATCH = 8\n",
    "MAX_TOKENS_PER_BATCH = 2000  # approximate per-batch token budget\n",
    "MAX_RESIDUES_PER_CHUNK = 1000  # split very long sequences\n",
    "MAX_TOKENS_PER_BATCH = 2000  # approximate per-batch token budget\n",
    "\n",
    "ESM_MODEL = 'esm2_t6_8M_UR50D'\n",
    "META_KEYS = ['source', 'gene', 'entry', 'file', 'idx']\n",
    "\n",
    "for fasta_path in [POS_FASTA, NEG_FASTA]:\n",
    "    if not fasta_path.exists():\n",
    "        raise FileNotFoundError(f\"Missing FASTA file: {fasta_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839e02e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "VALID_RESIDUES = set(\"ACDEFGHIKLMNPQRSTVWYBJOUXZ\")\n",
    "\n",
    "def is_valid_sequence(seq: str) -> bool:\n",
    "    return set(seq.upper()) <= VALID_RESIDUES\n",
    "\n",
    "\n",
    "def parse_header_metadata(header: str) -> dict:\n",
    "    parts = header.split('|')\n",
    "    meta = {'source': parts[0]}\n",
    "    for part in parts[1:]:\n",
    "        if '=' in part:\n",
    "            key, value = part.split('=', 1)\n",
    "            meta[key.strip()] = value.strip()\n",
    "    for key in META_KEYS:\n",
    "        meta.setdefault(key, 'NA')\n",
    "    return meta\n",
    "\n",
    "\n",
    "def read_fasta_with_metadata(file_path: Path, label: int) -> pd.DataFrame:\n",
    "    records = []\n",
    "    for record in SeqIO.parse(str(file_path), 'fasta'):\n",
    "        meta = parse_header_metadata(record.description)\n",
    "        records.append({\n",
    "            **meta,\n",
    "            'header': record.description,\n",
    "            'sequence': str(record.seq),\n",
    "            'label': label\n",
    "        })\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "\n",
    "positive_df = read_fasta_with_metadata(POS_FASTA, 1)\n",
    "negative_df = read_fasta_with_metadata(NEG_FASTA, 0)\n",
    "print(f\"Loaded {len(positive_df)} positive and {len(negative_df)} negative representatives\")\n",
    "\n",
    "# Combine, shuffle (for downstream ML), and index sequences\n",
    "combined_df = pd.concat([positive_df, negative_df], ignore_index=True)\n",
    "valid_mask = combined_df['sequence'].str.upper().apply(lambda s: set(s) <= VALID_RESIDUES)\n",
    "invalid = combined_df[~valid_mask]\n",
    "if len(invalid):\n",
    "    print(f'Removed {len(invalid)} sequences with non-standard residues')\n",
    "combined_df = combined_df[valid_mask].reset_index(drop=True)\n",
    "sequence_df = combined_df.sample(frac=1.0, random_state=random_seed).reset_index(drop=True)\n",
    "sequence_df.insert(0, 'sequence_id', range(len(sequence_df)))\n",
    "sequence_df['sequence_length'] = sequence_df['sequence'].str.len()\n",
    "\n",
    "metadata_columns = ['sequence_id', 'label'] + META_KEYS + ['header', 'sequence_length']\n",
    "metadata_df = sequence_df[metadata_columns].copy()\n",
    "print(f\"Unique genes represented: {metadata_df['gene'].nunique()}\")\n",
    "print(sequence_df.groupby('label')['sequence_length'].describe()[['mean', 'min', 'max']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03c1ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_loader = getattr(esm.pretrained, ESM_MODEL)\n",
    "model, alphabet = model_loader()\n",
    "model = model.eval().to(device)\n",
    "repr_layer = model.num_layers\n",
    "batch_converter = alphabet.get_batch_converter()\n",
    "\n",
    "\n",
    "def chunked_rows(df: pd.DataFrame):\n",
    "    for row in df.itertuples():\n",
    "        sequence = row.sequence\n",
    "        if len(sequence) <= MAX_RESIDUES_PER_CHUNK:\n",
    "            yield row, sequence\n",
    "        else:\n",
    "            for start in range(0, len(sequence), MAX_RESIDUES_PER_CHUNK):\n",
    "                yield row, sequence[start:start + MAX_RESIDUES_PER_CHUNK]\n",
    "\n",
    "\n",
    "def batch_iter(df: pd.DataFrame):\n",
    "    batch_rows = []\n",
    "    token_budget = 0\n",
    "    for row, chunk_seq in chunked_rows(df):\n",
    "        length = len(chunk_seq) + 2  # account for BOS/EOS tokens\n",
    "        if batch_rows and (\n",
    "            len(batch_rows) >= MAX_SEQUENCES_PER_BATCH or\n",
    "            token_budget + length > MAX_TOKENS_PER_BATCH\n",
    "        ):\n",
    "            yield batch_rows\n",
    "            batch_rows = []\n",
    "            token_budget = 0\n",
    "        batch_rows.append((row, chunk_seq))\n",
    "        token_budget += length\n",
    "    if batch_rows:\n",
    "        yield batch_rows\n",
    "\n",
    "\n",
    "def embed_dataframe(df: pd.DataFrame) -> np.ndarray:\n",
    "    embeddings = np.zeros((len(df), model.embed_dim), dtype=np.float32)\n",
    "    chunk_counts = np.zeros(len(df), dtype=np.int32)\n",
    "    processed = 0\n",
    "    for batch_rows in batch_iter(df):\n",
    "        batch_ids = [str(row.sequence_id) for row, _ in batch_rows]\n",
    "        batch_seqs = [seq for _, seq in batch_rows]\n",
    "        batch_data = list(zip(batch_ids, batch_seqs))\n",
    "        _, _, batch_tokens = batch_converter(batch_data)\n",
    "        batch_tokens = batch_tokens.to(device)\n",
    "        with torch.no_grad():\n",
    "            results = model(batch_tokens, repr_layers=[repr_layer], return_contacts=False)\n",
    "        token_representations = results['representations'][repr_layer]\n",
    "        seq_representations = token_representations[:, 1:-1].mean(1).cpu().numpy()\n",
    "        for (row, _), embedding in zip(batch_rows, seq_representations):\n",
    "            embeddings[row.Index] += embedding\n",
    "            chunk_counts[row.Index] += 1\n",
    "        processed += len(batch_rows)\n",
    "        if processed % 200 == 0:\n",
    "            print(f\"Processed {processed} chunks\")\n",
    "    if not np.all(chunk_counts):\n",
    "        missing = np.where(chunk_counts == 0)[0]\n",
    "        raise RuntimeError(f\"Missing embeddings for rows: {missing[:10]} ...\")\n",
    "    embeddings /= chunk_counts[:, None]\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "embeddings = embed_dataframe(sequence_df)\n",
    "labels = sequence_df['label'].to_numpy(dtype=np.int8)\n",
    "np.save(OUTPUT_DIR / 'esm_features.npy', embeddings)\n",
    "np.save(OUTPUT_DIR / 'labels.npy', labels)\n",
    "metadata_df.to_csv(OUTPUT_DIR / 'sequence_metadata.csv', index=False)\n",
    "metadata_df.to_parquet(OUTPUT_DIR / 'sequence_metadata.parquet', index=False)\n",
    "\n",
    "print('Saved embeddings to', OUTPUT_DIR / 'esm_features.npy')\n",
    "print('Saved labels to', OUTPUT_DIR / 'labels.npy')\n",
    "print('Saved metadata to', OUTPUT_DIR / 'sequence_metadata.(csv|parquet)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0911eb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def summarize_gene_performance(predictions, metadata: pd.DataFrame, threshold: float = 0.5) -> pd.DataFrame:\n",
    "    \"\"\"Link downstream model predictions back to genes for diagnostics.\"\"\"\n",
    "    scores = np.asarray(predictions)\n",
    "    if len(scores) != len(metadata):\n",
    "        raise ValueError('Predictions and metadata must have the same length')\n",
    "\n",
    "    if scores.ndim == 2:\n",
    "        if scores.shape[1] == 2:\n",
    "            scores = scores[:, 1]\n",
    "        elif scores.shape[1] == 1:\n",
    "            scores = scores[:, 0]\n",
    "        else:\n",
    "            raise ValueError('Unsupported prediction shape; expected (n,), (n,1) or (n,2)')\n",
    "\n",
    "    df = metadata.copy().reset_index(drop=True)\n",
    "    df['prediction_score'] = scores\n",
    "    df['pred_label'] = (df['prediction_score'] >= threshold).astype(int)\n",
    "    df['correct'] = (df['pred_label'] == df['label'])\n",
    "\n",
    "    summary = (\n",
    "        df.groupby(['gene', 'label'])\n",
    "          .agg(\n",
    "              n_sequences=('sequence_id', 'count'),\n",
    "              accuracy=('correct', 'mean')\n",
    "          )\n",
    "          .reset_index()\n",
    "          .sort_values('accuracy', ascending=False)\n",
    "    )\n",
    "    return summary\n",
    "\n",
    "\n",
    "# Example usage after training a classifier:\n",
    "# preds = trained_model.predict_proba(embeddings)[:, 1]\n",
    "# gene_perf = summarize_gene_performance(preds, metadata_df)\n",
    "# gene_perf.head()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNJP0JmBW36n7iP46h52vsE",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
